import sys
import os
import requests
import shutil
from bs4 import BeautifulSoup
import colorama
from colorama import Fore, Style
from tqdm import tqdm
import re

# Initialiser colorama pour Windows
colorama.init()

def print_banner():
    """Affiche le banner WebCloner en vert avec des caractÃ¨res spÃ©ciaux"""
    banner = f"""
{Fore.GREEN}
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                                  â•‘
â•‘    â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â•‘
â•‘    â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•— â•‘
â•‘    â–ˆâ–ˆâ•‘ â–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â•‘
â•‘    â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•— â•‘
â•‘    â•šâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘ â•‘
â•‘     â•šâ•â•â•â•šâ•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â• â•‘
â•‘                                                                                  â•‘
â•‘                    ğŸŒ Site Web Cloner Tool ğŸŒ                                    â•‘
â•‘                                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{Style.RESET_ALL}
"""
    print(banner)

def get_user_input():
    """Demande l'URL et le nom du fichier Ã  l'utilisateur"""
    print(f"{Fore.CYAN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{Style.RESET_ALL}")
    print(f"{Fore.CYAN}â•‘                    Configuration requise                     â•‘{Style.RESET_ALL}")
    print(f"{Fore.CYAN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Style.RESET_ALL}")
    
    # Demander l'URL du site
    while True:
        site_url = input(f"{Fore.YELLOW}ğŸŒ Entrez l'URL du site Ã  cloner (ex: https://example.com): {Style.RESET_ALL}").strip()
        if site_url:
            # Ajouter https:// si pas de protocole spÃ©cifiÃ©
            if not site_url.startswith(('http://', 'https://')):
                site_url = 'https://' + site_url
            break
        else:
            print(f"{Fore.RED}âŒ L'URL ne peut pas Ãªtre vide. Veuillez rÃ©essayer.{Style.RESET_ALL}")
    
    # Demander le type de clonage
    print(f"\n{Fore.CYAN}ğŸ“‹ Choisissez le type de clonage:{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}1. ğŸ“„ Page unique (copie uniquement la page actuelle){Style.RESET_ALL}")
    print(f"{Fore.YELLOW}2. ğŸŒ Site complet (copie toutes les pages liÃ©es){Style.RESET_ALL}")
    
    while True:
        clone_type = input(f"{Fore.YELLOW}Votre choix (1 ou 2): {Style.RESET_ALL}").strip()
        if clone_type in ['1', '2']:
            clone_type = int(clone_type)
            break
        else:
            print(f"{Fore.RED}âŒ Veuillez entrer 1 ou 2.{Style.RESET_ALL}")
    
    # Demander le nom du dossier de destination
    while True:
        project_name = input(f"{Fore.YELLOW}ğŸ“ Entrez le nom du dossier de destination: {Style.RESET_ALL}").strip()
        if project_name:
            # Nettoyer le nom du dossier (enlever les caractÃ¨res spÃ©ciaux)
            project_name = re.sub(r'[<>:"/\\|?*]', '_', project_name)
            break
        else:
            print(f"{Fore.RED}âŒ Le nom du dossier ne peut pas Ãªtre vide. Veuillez rÃ©essayer.{Style.RESET_ALL}")
    
    return site_url, project_name, clone_type

# Afficher le banner
print_banner()

# Obtenir les entrÃ©es utilisateur
site_name, project_name, clone_type = get_user_input()

print(f"\n{Fore.GREEN}âœ… Configuration validÃ©e:{Style.RESET_ALL}")
print(f"   ğŸŒ Site: {Fore.CYAN}{site_name}{Style.RESET_ALL}")
print(f"   ğŸ“ Dossier: {Fore.CYAN}{project_name}{Style.RESET_ALL}")
if clone_type == 1:
    print(f"   ğŸ“„ Type: {Fore.CYAN}Page unique{Style.RESET_ALL}")
else:
    print(f"   ğŸŒ Type: {Fore.CYAN}Site complet{Style.RESET_ALL}")
print(f"\n{Fore.YELLOW}ğŸš€ DÃ©marrage du clonage...{Style.RESET_ALL}\n")

base_dir = os.getcwd()

project_path = "../" + project_name
os.makedirs(project_path, exist_ok=True)

visited_links = []
error_links = []
total_files = 0
downloaded_files = 0

def update_progress_bar():
    """Met Ã  jour la barre de progression globale"""
    global downloaded_files, total_files
    if total_files > 0:
        progress = (downloaded_files / total_files) * 100
        print(f"\r{Fore.CYAN}ğŸ“Š Progression globale: {Fore.GREEN}{downloaded_files}/{total_files}{Style.RESET_ALL} fichiers ({progress:.1f}%)", end="", flush=True)

def save(bs, element, check):
    global downloaded_files, total_files
    links = bs.find_all(element)
    
    # Compter les fichiers Ã  tÃ©lÃ©charger
    files_to_download = []
    for l in links:
        href = l.get("href")
        if href is not None and href not in visited_links:
            if check in href:
                files_to_download.append(href)
    
    if files_to_download:
        print(f"{Fore.YELLOW}ğŸ¨ TÃ©lÃ©chargement des fichiers {check}...{Style.RESET_ALL}")
        
        # Barre de progression pour les fichiers CSS/JS
        with tqdm(total=len(files_to_download), desc=f"ğŸ“„ Fichiers {check}", 
                 bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
            
            for href in files_to_download:
                if href not in visited_links:
                    pbar.set_description(f"ğŸ“„ {check}: {os.path.basename(href)}")
                    
                    if "//" in href:
                        path_s = href.split("/")
                        file_name = ""
                        for i in range(3, len(path_s)):
                            file_name = file_name + "/" + path_s[i]
                    else:
                        file_name = href

                    l = site_name + file_name

                    try:
                        r = requests.get(l)
                    except requests.exceptions.ConnectionError:
                        error_links.append(l)
                        pbar.update(1)
                        continue

                    if r.status_code != 200:
                        error_links.append(l)
                        pbar.update(1)
                        continue

                    os.makedirs(os.path.dirname(project_path + file_name.split("?")[0]), exist_ok=True)
                    with open(project_path + file_name.split("?")[0], "wb") as f:
                        f.write(r.text.encode('utf-8'))
                        f.close()

                    visited_links.append(l)
                    downloaded_files += 1
                    update_progress_bar()
                    pbar.update(1)

def save_assets(html_text):
    global downloaded_files, total_files
    bs = BeautifulSoup(html_text, "html.parser")
    
    # Traiter les CSS et JS
    save(bs=bs, element="link", check=".css")
    save(bs=bs, element="script", check=".js")

    # Traiter les images
    links = bs.find_all("img")
    images_to_download = []
    
    for l in links:
        href = l.get("src")
        if href is not None and href not in visited_links:
            images_to_download.append(href)
    
    if images_to_download:
        print(f"{Fore.YELLOW}ğŸ–¼ï¸  TÃ©lÃ©chargement des images...{Style.RESET_ALL}")
        
        # Barre de progression pour les images
        with tqdm(total=len(images_to_download), desc="ğŸ–¼ï¸ Images", 
                 bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
            
            for href in images_to_download:
                if href not in visited_links:
                    pbar.set_description(f"ğŸ–¼ï¸ Image: {os.path.basename(href)}")
                    
                    if "//" in href:
                        path_s = href.split("/")
                        file_name = ""
                        for i in range(3, len(path_s)):
                            file_name = file_name + "/" + path_s[i]
                    else:
                        file_name = href

                    l = site_name + file_name

                    try:
                        r = requests.get(l, stream=True)
                    except requests.exceptions.ConnectionError:
                        error_links.append(l)
                        pbar.update(1)
                        continue

                    if r.status_code != 200:
                        error_links.append(l)
                        pbar.update(1)
                        continue

                    os.makedirs(os.path.dirname(project_path + file_name.split("?")[0]), exist_ok=True)
                    with open(project_path + file_name.split("?")[0], "wb") as f:
                        shutil.copyfileobj(r.raw, f)

                    visited_links.append(l)
                    downloaded_files += 1
                    update_progress_bar()
                    pbar.update(1)

def crawl(link):
    global total_files, downloaded_files
    if "http://" not in link and "https://" not in link:
        link = site_name + link

    if site_name in link and link not in visited_links:
        print(f"{Fore.MAGENTA}ğŸ”— Exploration: {Fore.CYAN}{link}{Style.RESET_ALL}")

        path_s = link.split("/")
        file_name = ""
        for i in range(3, len(path_s)):
            file_name = file_name + "/" + path_s[i]

        if file_name[len(file_name) - 1] != "/":
            file_name = file_name + "/"

        try:
            r = requests.get(link)
        except requests.exceptions.ConnectionError:
            print(f"{Fore.RED}âŒ Erreur de connexion au site{Style.RESET_ALL}")
            sys.exit(1)

        if r.status_code != 200:
            print(f"{Fore.RED}âŒ RÃ©ponse invalide du serveur{Style.RESET_ALL}")
            sys.exit(1)
        
        print(f"{Fore.GREEN}ğŸ“ CrÃ©ation: {project_path + file_name}index.html{Style.RESET_ALL}")
        os.makedirs(os.path.dirname(project_path + file_name.split("?")[0]), exist_ok=True)
        with open(project_path + file_name.split("?")[0] + "index.html", "wb") as f:
            text = r.text.replace(site_name, project_name)
            f.write(text.encode('utf-8'))
            f.close()

        visited_links.append(link)
        downloaded_files += 1
        update_progress_bar()

        # Analyser le contenu pour compter les fichiers Ã  tÃ©lÃ©charger
        soup = BeautifulSoup(r.text, "html.parser")
        
        # Compter les fichiers CSS, JS et images
        css_files = [l.get("href") for l in soup.find_all("link") if l.get("href") and ".css" in l.get("href")]
        js_files = [l.get("src") for l in soup.find_all("script") if l.get("src") and ".js" in l.get("src")]
        img_files = [l.get("src") for l in soup.find_all("img") if l.get("src")]
        
        # Ajouter au total global
        total_files += len(css_files) + len(js_files) + len(img_files)

        save_assets(r.text)

        # Ne suivre les liens que si on clone le site complet
        if clone_type == 2:
            for link in soup.find_all('a'):
                try:
                    crawl(link.get("href"))
                except:
                    error_links.append(link.get("href"))

def clone_single_page():
    """Clone uniquement la page principale sans suivre les liens"""
    global total_files, downloaded_files
    
    print(f"{Fore.YELLOW}ğŸ“„ Clonage de la page unique: {site_name}{Style.RESET_ALL}\n")
    
    try:
        r = requests.get(site_name)
    except requests.exceptions.ConnectionError:
        print(f"{Fore.RED}âŒ Erreur de connexion au site{Style.RESET_ALL}")
        sys.exit(1)

    if r.status_code != 200:
        print(f"{Fore.RED}âŒ RÃ©ponse invalide du serveur{Style.RESET_ALL}")
        sys.exit(1)
    
    print(f"{Fore.GREEN}ğŸ“ CrÃ©ation: {project_path}/index.html{Style.RESET_ALL}")
    os.makedirs(project_path, exist_ok=True)
    with open(project_path + "/index.html", "wb") as f:
        text = r.text.replace(site_name, project_name)
        f.write(text.encode('utf-8'))
        f.close()

    visited_links.append(site_name)
    downloaded_files += 1
    update_progress_bar()

    # Analyser le contenu pour compter les fichiers Ã  tÃ©lÃ©charger
    soup = BeautifulSoup(r.text, "html.parser")
    
    # Compter les fichiers CSS, JS et images
    css_files = [l.get("href") for l in soup.find_all("link") if l.get("href") and ".css" in l.get("href")]
    js_files = [l.get("src") for l in soup.find_all("script") if l.get("src") and ".js" in l.get("src")]
    img_files = [l.get("src") for l in soup.find_all("img") if l.get("src")]
    
    # Ajouter au total global
    total_files += len(css_files) + len(js_files) + len(img_files)

    save_assets(r.text)

print(f"{Fore.YELLOW}ğŸŒ DÃ©marrage du clonage de {site_name}...{Style.RESET_ALL}\n")

# Initialiser le compteur total
total_files = 1  # Au moins la page principale
downloaded_files = 0

# Choisir le type de clonage
if clone_type == 1:
    clone_single_page()
else:
    crawl(site_name + "/")

print(f"\n\n{Fore.GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{Style.RESET_ALL}")
print(f"{Fore.GREEN}â•‘                    RÃ‰SUMÃ‰ DU CLONAGE                         â•‘{Style.RESET_ALL}")
print(f"{Fore.GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{Style.RESET_ALL}")

if clone_type == 1:
    print(f"\n{Fore.CYAN}ğŸ“„ Type de clonage: {Fore.GREEN}Page unique{Style.RESET_ALL}")
else:
    print(f"\n{Fore.CYAN}ğŸŒ Type de clonage: {Fore.GREEN}Site complet{Style.RESET_ALL}")

print(f"\n{Fore.CYAN}ğŸ“Š Liens traitÃ©s ({len(visited_links)}):{Style.RESET_ALL}")
for link in visited_links:
    print(f"{Fore.GREEN}   âœ… {link}{Style.RESET_ALL}")

if error_links:
    print(f"\n{Fore.RED}âŒ Liens en erreur ({len(error_links)}):{Style.RESET_ALL}")
    for link in error_links:
        print(f"{Fore.RED}   âŒ {link}{Style.RESET_ALL}")

if clone_type == 1:
    print(f"\n{Fore.GREEN}ğŸ‰ Clonage de la page terminÃ© ! Le fichier a Ã©tÃ© sauvegardÃ© dans: {Fore.CYAN}{project_path}/index.html{Style.RESET_ALL}")
else:
    print(f"\n{Fore.GREEN}ğŸ‰ Clonage du site terminÃ© ! Le site a Ã©tÃ© sauvegardÃ© dans: {Fore.CYAN}{project_path}{Style.RESET_ALL}")